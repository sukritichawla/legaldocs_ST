{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukritichawla/legaldocs_ST/blob/main/notebooks/00_colab_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05XKaTIwcTJ"
      },
      "source": [
        "# Colab Setup Notebook\n",
        "Run these cells step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWrerWVCwcTL",
        "outputId": "80299f12-292c-4cbe-f5d0-ad539eaae64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep  2 15:21:03 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi || echo 'No GPU available (OK, slower runs)'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installs required libs (may take 1–3 minutes)\n",
        "!pip -q install -U transformers datasets sentencepiece accelerate huggingface_hub python-dotenv feedparser beautifulsoup4 sacrebleu rouge-score gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpZ2HRrkw9nE",
        "outputId": "af2f590b-f454-405f-c142-63795cef87fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "token = getpass(\"Enter your GitHub PAT (keeps input hidden): \")\n",
        "USERNAME = \"sukritichawla\"    # replace if needed\n",
        "REPO = \"legaldocs_ST\"         # replace if needed\n",
        "REPO_URL = f\"https://{token}@github.com/{USERNAME}/{REPO}.git\"\n",
        "!git clone $REPO_URL\n",
        "%cd $REPO\n",
        "!ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qobLyADlw-e2",
        "outputId": "064da1c9-65e3-4769-d2a9-9b0cc6ead03e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub PAT (keeps input hidden): ··········\n",
            "Cloning into 'legaldocs_ST'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), done.\n",
            "/content/legaldocs_ST\n",
            "total 24\n",
            "drwxr-xr-x 4 root root 4096 Sep  2 14:13 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 14:13 ..\n",
            "drwxr-xr-x 8 root root 4096 Sep  2 14:13 .git\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 14:13 notebooks\n",
            "-rw-r--r-- 1 root root   14 Sep  2 14:13 README.md\n",
            "-rw-r--r-- 1 root root 1920 Sep  2 14:13 starter_clean.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/legaldocs_ST\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuKpTe8KxduY",
        "outputId": "f02ccd86-a0d5-4851-b4a2-7de4d3aab3ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/legaldocs_ST'\n",
            "/content\n",
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Aug 28 13:42 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 15:19 ..\n",
            "drwxr-xr-x 4 root root 4096 Aug 28 13:42 .config\n",
            "drwxr-xr-x 1 root root 4096 Aug 28 13:43 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o starter_clean.zip -d .\n",
        "!ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLIJkpcByA8o",
        "outputId": "d41f50fc-15c1-4eaf-e41d-33d6e52e033f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  starter_clean.zip\n",
            "  inflating: ./README.md             \n",
            " extracting: ./scripts/fetch_rss_to_csv.py  \n",
            "total 44\n",
            "drwxr-xr-x 9 root root 4096 Sep  2 14:22 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 14:13 ..\n",
            "drwxr-xr-x 4 root root 4096 Sep  2 13:23 data\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 13:23 docs\n",
            "drwxr-xr-x 8 root root 4096 Sep  2 14:22 .git\n",
            "drwxr-xr-x 3 root root 4096 Sep  2 13:23 .github\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 14:13 notebooks\n",
            "-rw-r--r-- 1 root root   71 Sep  2 13:23 README.md\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 14:22 scripts\n",
            "drwxr-xr-x 3 root root 4096 Sep  2 13:23 src\n",
            "-rw-r--r-- 1 root root 1920 Sep  2 14:13 starter_clean.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"sukritichawla77@gmail.com\"\n",
        "!git config --global user.name \"Sukriti Chawla\"\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"add starter scaffold\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsQpdRi3yG8R",
        "outputId": "4da49ae9-08f4-49e8-b231-ceec6a625cc2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make the standard folders\n",
        "!mkdir -p data/raw data/processed docs src/pipeline .github/workflows\n",
        "\n",
        "# add placeholder files so git tracks the folders\n",
        "!echo \"# raw data goes here\" > data/raw/README.md\n",
        "!echo \"# processed data goes here\" > data/processed/README.md\n",
        "!echo \"# docs\" > docs/README.md\n",
        "!echo \"# pipeline code\" > src/pipeline/README.md\n",
        "!echo \"name: CI\" > .github/workflows/ci.yml\n",
        "\n",
        "# stage and push\n",
        "!git add .\n",
        "!git commit -m \"add project scaffold folders\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlfrRBNMy44g",
        "outputId": "a27f53fe-b72e-437f-a2fd-305d494faede"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 460c11e] add project scaffold folders\n",
            " 5 files changed, 5 insertions(+)\n",
            " create mode 100644 .github/workflows/ci.yml\n",
            " create mode 100644 data/processed/README.md\n",
            " create mode 100644 data/raw/README.md\n",
            " create mode 100644 docs/README.md\n",
            " create mode 100644 src/pipeline/README.md\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (15/15), 996 bytes | 996.00 KiB/s, done.\n",
            "Total 15 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/sukritichawla/legaldocs_ST.git\n",
            " \u001b[31m! [remote rejected]\u001b[m main -> main (refusing to allow a Personal Access Token to create or update workflow `.github/workflows/ci.yml` without `workflow` scope)\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/sukritichawla/legaldocs_ST.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add data docs src\n",
        "!git commit -m \"add project scaffold folders without CI\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl1ODUJsz_P1",
        "outputId": "6d1b5e46-bdf9-4df6-8e6c-c26fa66c5186"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/legaldocs_ST\n",
        "!ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtICF6-NA0eH",
        "outputId": "cec3e8d4-4953-4f8e-acd1-22420b50c9c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/legaldocs_ST'\n",
            "/content\n",
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Aug 28 13:42 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 15:19 ..\n",
            "drwxr-xr-x 4 root root 4096 Aug 28 13:42 .config\n",
            "drwxr-xr-x 1 root root 4096 Aug 28 13:43 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf legaldocs_ST"
      ],
      "metadata": {
        "id": "yiVLNKI6BE57"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "token = getpass(\"Enter your GitHub PAT (keeps input hidden): \")\n",
        "USERNAME = \"sukritichawla\"    # replace if needed\n",
        "REPO = \"legaldocs_ST\"         # replace if needed\n",
        "REPO_URL = f\"https://{token}@github.com/{USERNAME}/{REPO}.git\"\n",
        "!git clone $REPO_URL\n",
        "%cd $REPO\n",
        "!ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olo26nCTB1Dn",
        "outputId": "cdf9d1d4-a0c9-4969-897f-8b64a8be3b87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub PAT (keeps input hidden): ··········\n",
            "Cloning into 'legaldocs_ST'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), done.\n",
            "/content/legaldocs_ST\n",
            "total 28\n",
            "drwxr-xr-x 5 root root 4096 Sep  2 15:24 .\n",
            "drwxr-xr-x 1 root root 4096 Sep  2 15:24 ..\n",
            "drwxr-xr-x 8 root root 4096 Sep  2 15:24 .git\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 15:24 notebooks\n",
            "-rw-r--r-- 1 root root   71 Sep  2 15:24 README.md\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 15:24 scripts\n",
            "-rw-r--r-- 1 root root 1920 Sep  2 15:24 starter_clean.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLyXCCV3B5CF",
        "outputId": "5737a3e0-e4bc-4a36-c532-6b5c7653b71c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/legaldocs_ST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/raw data/processed docs src/pipeline\n",
        "\n",
        "# add placeholder files so Git tracks folders\n",
        "!echo \"# raw data\" > data/raw/README.md\n",
        "!echo \"# processed data\" > data/processed/README.md\n",
        "!echo \"# docs\" > docs/README.md\n",
        "!echo \"# pipeline code\" > src/pipeline/README.md\n",
        "\n",
        "# add requirements file\n",
        "!echo \"transformers\\ndatasets\\nsentencepiece\\naccelerate\\nhuggingface_hub\\npandas\\nnumpy\\ntqdm\\nbeautifulsoup4\\nfeedparser\\nsacrebleu\\nrouge-score\\ngradi\" > requirements.txt\n",
        "\n",
        "# commit & push\n",
        "!git config --global user.email \"sukritichawla77@gmail.com\"\n",
        "!git config --global user.name \"Sukriti Chawla\"\n",
        "!git add .\n",
        "!git commit -m \"add scaffold folders + requirements\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XazHHy-1B_nU",
        "outputId": "2a1f06d6-5e6e-49ad-c12c-8d0b5525d236"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 39f8789] add scaffold folders + requirements\n",
            " 5 files changed, 5 insertions(+)\n",
            " create mode 100644 data/processed/README.md\n",
            " create mode 100644 data/raw/README.md\n",
            " create mode 100644 docs/README.md\n",
            " create mode 100644 requirements.txt\n",
            " create mode 100644 src/pipeline/README.md\n",
            "Enumerating objects: 14, done.\n",
            "Counting objects: 100% (14/14), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (13/13), 1001 bytes | 1001.00 KiB/s, done.\n",
            "Total 13 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/sukritichawla/legaldocs_ST.git\n",
            "   036af73..39f8789  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Enter your HF token (starts with hf_): \")\n",
        "login(token=hf_token)\n",
        "\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "print(\"HF login done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAUmqyb_CIXK",
        "outputId": "042fa401-1eaa-4de3-83fc-7621f20329d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your HF token (starts with hf_): ··········\n",
            "HF login done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/fetch_rss_to_csv.py || echo \"RSS fetch script placeholder\"\n",
        "!ls -la data/raw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6wfUIBjCxwo",
        "outputId": "eed5b7cd-6b40-454d-8490-a8bc09f2c8d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fetch rss placeholder\n",
            "total 12\n",
            "drwxr-xr-x 2 root root 4096 Sep  2 15:25 .\n",
            "drwxr-xr-x 4 root root 4096 Sep  2 15:25 ..\n",
            "-rw-r--r-- 1 root root   11 Sep  2 15:26 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main"
      ],
      "metadata": {
        "id": "sDZKuMDyDx9W",
        "outputId": "810758e6-5e05-44b5-e0cc-7ada6fa58733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:  14% (1/7)\u001b[K\rremote: Compressing objects:  28% (2/7)\u001b[K\rremote: Compressing objects:  42% (3/7)\u001b[K\rremote: Compressing objects:  57% (4/7)\u001b[K\rremote: Compressing objects:  71% (5/7)\u001b[K\rremote: Compressing objects:  85% (6/7)\u001b[K\rremote: Compressing objects: 100% (7/7)\u001b[K\rremote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  11% (1/9)\rUnpacking objects:  22% (2/9)\rUnpacking objects:  33% (3/9)\rUnpacking objects:  44% (4/9)\rUnpacking objects:  55% (5/9)\rUnpacking objects:  66% (6/9)\rUnpacking objects:  77% (7/9)\rUnpacking objects:  88% (8/9)\rUnpacking objects: 100% (9/9)\rUnpacking objects: 100% (9/9), 5.56 KiB | 1.85 MiB/s, done.\n",
            "From https://github.com/sukritichawla/legaldocs_ST\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   39f8789..e1cfeb7  main       -> origin/main\n",
            "Updating 39f8789..e1cfeb7\n",
            "Fast-forward\n",
            " notebooks/00_colab_setup.ipynb               | 612 \u001b[32m+++++++++++++++++++++++++\u001b[m\u001b[31m--\u001b[m\n",
            " src/pipeline/summarize_simplify_translate.py |  19 \u001b[32m+\u001b[m\n",
            " 2 files changed, 600 insertions(+), 31 deletions(-)\n",
            " create mode 100644 src/pipeline/summarize_simplify_translate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/pipeline/summarize_simplify_translate.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK6A_LZaC5mG",
        "outputId": "3837db1b-3a93-495d-d579-6fab93e8835a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-02 15:33:06.255548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756827186.275303    3567 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756827186.281242    3567 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756827186.297068    3567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756827186.297095    3567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756827186.297099    3567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756827186.297104    3567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-02 15:33:06.301643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 553/553 [00:00<00:00, 4.05MB/s]\n",
            "pytorch_model.bin: 100% 1.20G/1.20G [00:14<00:00, 84.7MB/s]\n",
            "model.safetensors:   0% 0.00/1.20G [00:00<?, ?B/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 712kB/s]\n",
            "model.safetensors:   6% 71.6M/1.20G [00:01<00:17, 63.7MB/s]\n",
            "tokenizer_config.json: 100% 82.0/82.0 [00:00<00:00, 377kB/s]\n",
            "model.safetensors:  21% 248M/1.20G [00:03<00:09, 99.3MB/s]\n",
            "spiece.model: 100% 4.31M/4.31M [00:00<00:00, 35.6MB/s]\n",
            "model.safetensors:  32% 388M/1.20G [00:04<00:08, 92.0MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 744kB/s]\n",
            "model.safetensors:  39% 474M/1.20G [00:06<00:12, 58.2MB/s]You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "model.safetensors:  44% 533M/1.20G [00:07<00:10, 66.2MB/s]/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "model.safetensors:  50% 597M/1.20G [00:09<00:13, 45.8MB/s]\n",
            "config.json: 1.39kB [00:00, 2.96MB/s]\n",
            "model.safetensors:  61% 731M/1.20G [00:10<00:06, 67.2MB/s]\n",
            "pytorch_model.bin:   0% 0.00/306M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:  66% 798M/1.20G [00:13<00:09, 42.3MB/s]\n",
            "model.safetensors:  72% 865M/1.20G [00:13<00:05, 58.9MB/s]\n",
            "pytorch_model.bin:  21% 62.9M/306M [00:02<00:07, 32.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  27% 83.9M/306M [00:06<00:20, 10.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  34% 105M/306M [00:06<00:12, 16.3MB/s] \u001b[A\n",
            "model.safetensors:  83% 1.00G/1.20G [00:23<00:09, 20.9MB/s]\n",
            "pytorch_model.bin:  45% 136M/306M [00:12<00:23, 7.33MB/s]\u001b[A\n",
            "model.safetensors:  89% 1.07G/1.20G [00:24<00:04, 28.5MB/s]\n",
            "pytorch_model.bin:  51% 157M/306M [00:13<00:13, 11.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  55% 168M/306M [00:13<00:09, 14.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  58% 178M/306M [00:13<00:06, 18.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  62% 189M/306M [00:13<00:04, 23.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  65% 199M/306M [00:13<00:03, 29.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  69% 210M/306M [00:13<00:02, 36.1MB/s]\u001b[A\n",
            "model.safetensors:  94% 1.13G/1.20G [00:24<00:01, 35.1MB/s]\n",
            "pytorch_model.bin:  79% 241M/306M [00:13<00:01, 63.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  86% 262M/306M [00:14<00:00, 79.0MB/s]\u001b[A\n",
            "model.safetensors: 100% 1.20G/1.20G [00:25<00:00, 47.5MB/s]\n",
            "\n",
            "pytorch_model.bin: 100% 306M/306M [00:14<00:00, 21.2MB/s]\n",
            "generation_config.json: 100% 293/293 [00:00<00:00, 2.37MB/s]\n",
            "model.safetensors:  62% 189M/306M [00:00<00:00, 335MB/s]\n",
            "tokenizer_config.json: 100% 44.0/44.0 [00:00<00:00, 273kB/s]\n",
            "model.safetensors: 100% 306M/306M [00:00<00:00, 323MB/s]\n",
            "source.spm: 100% 812k/812k [00:00<00:00, 33.8MB/s]\n",
            "target.spm: 100% 1.07M/1.07M [00:00<00:00, 91.3MB/s]\n",
            "vocab.json: 2.10MB [00:00, 13.8MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cuda:0\n",
            "Your max_length is set to 60, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "{'summary_en': '<extra_id_0>.) <extra_id_36> grants for rural schools.', 'summary_hi': '<ROD_id>. <R_de_id> ग्रामीण स्कूलों के लिए प्रदान करता है.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcvUMLL6DPDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}